{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DAT341 - Assignment 5 - Group 69__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is formatted as such :\n",
    "\n",
    "- First, all the packages are imported and the data is loaded (first cell)\n",
    "- Then, the utility functions are defined such as :\n",
    "    - Training function for our models\n",
    "    - Evaluation function\n",
    "    - Blind test set prediction function\n",
    "- Finally, differents models and techniques will be tried in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Torchvision packages\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models\n",
    "\n",
    "# Importing data\n",
    "train_dir = 'a5_data/train'\n",
    "val_dir = 'a5_data/val'\n",
    "test_dir = 'a5_data/test_blind'\n",
    "\n",
    "# Transformations, normalization and augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = ImageFolder(val_dir, transform=transform)\n",
    "test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self,model,criterion,optimizer,features_location=None):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.features_location = features_location\n",
    "\n",
    "        if self.features_location is not None:\n",
    "            self.train_features = torch.load(f'{self.features_location}/train.pt')\n",
    "            self.val_features = torch.load(f'{self.features_location}/val.pt')\n",
    "            self.test_features = torch.load(f'{self.features_location}/test.pt')\n",
    "            \n",
    "        \n",
    "    # To evaluate our model on the validation set\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i,(images, labels) in tqdm(enumerate(val_loader)):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                if self.features_location is not None:\n",
    "                    images = self.val_features[i*32:(i*32 + labels.size(0))].to(device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy on validation set: {100 * correct / total}%')\n",
    "    \n",
    "    def predict(self,name):\n",
    "        if not os.path.exists('predictions'):\n",
    "            os.makedirs('predictions')\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for i,(images, labels) in tqdm(enumerate(test_loader)):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                if self.features_location is not None:\n",
    "                    images = self.test_features[i*32:(i*32 + labels.size(0))].to(device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predictions += ['MEL' if label == 0 else 'NV' for label in predicted]\n",
    "\n",
    "            # Save the predictions to a file\n",
    "            with open(f'predictions/{name}.txt', 'w') as f:\n",
    "                for item in predictions:\n",
    "                    f.write(\"%s\\n\" % item)\n",
    "                    \n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        # Training loop\n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for i,(images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                if self.features_location is not None:\n",
    "                    images = self.train_features[i*32:(i*32 + labels.size(0))].to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "        print(f'Training finished - Elapsed time: {time.time() - start_time} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Simple CNN - No fancy feature extractor or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.51190748173206\n",
      "Epoch 2, Loss: 0.4360037534242839\n",
      "Epoch 3, Loss: 0.41872741860240253\n",
      "Epoch 4, Loss: 0.40243225526157306\n",
      "Epoch 5, Loss: 0.39853007787495703\n",
      "Epoch 6, Loss: 0.3915621171988065\n",
      "Epoch 7, Loss: 0.3823899925496448\n",
      "Epoch 8, Loss: 0.3733556007419653\n",
      "Epoch 9, Loss: 0.3721242454366304\n",
      "Epoch 10, Loss: 0.36166907209365523\n",
      "Training finished - Elapsed time: 166.8477909564972 s.\n",
      "Accuracy on validation set: 80.03194888178913%\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleCNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 16 * 53 * 53)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "    \n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "Classifier1 = Classifier(model, criterion, optimizer)\n",
    "Classifier1.train()\n",
    "Classifier1.validate()\n",
    "Classifier1.predict('simpleCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5383621391965382\n",
      "Epoch 2, Loss: 0.3876084038125935\n",
      "Epoch 3, Loss: 0.36049412776581685\n",
      "Epoch 4, Loss: 0.33995049339325273\n",
      "Epoch 5, Loss: 0.32777240957638515\n",
      "Epoch 6, Loss: 0.32433458347225663\n",
      "Epoch 7, Loss: 0.3182627019004442\n",
      "Epoch 8, Loss: 0.3093501931769931\n",
      "Epoch 9, Loss: 0.30165900697755577\n",
      "Epoch 10, Loss: 0.30813395250495984\n",
      "Training finished - Elapsed time: 156.12875604629517 s.\n",
      "Accuracy on validation set: 86.26198083067092%\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained resnet18 model\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze it\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Modify the last layer of ResNet to match the input size of the first linear layer\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Identity()\n",
    "\n",
    "# Define the sequential model with ResNet and additional linear layers\n",
    "model = nn.Sequential(\n",
    "    resnet,\n",
    "    nn.Linear(num_ftrs, 512),   # First linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Linear(512, 256),        # Second linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Linear(256, 2)           # Output layer with output size of 2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "Classifier2 = Classifier(model, criterion, optimizer)\n",
    "Classifier2.train()\n",
    "Classifier2.validate()\n",
    "Classifier2.predict('Resnet18')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.42754049599170685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     24\u001b[0m Classifier3 \u001b[38;5;241m=\u001b[39m Classifier(model, criterion, optimizer)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mClassifier3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m Classifier3\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m     27\u001b[0m Classifier3\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG19\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36mClassifier.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 66\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining finished - Elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "\n",
    "# freeze all parameters\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "# Define the sequential model with VGG and additional linear layers\n",
    "model = nn.Sequential(\n",
    "    vgg,\n",
    "    nn.Flatten(),               # Flatten the output of VGG\n",
    "    nn.Linear(25088, 4096),     # First linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Dropout(0.5),            # Dropout with p=0.5\n",
    "    nn.Linear(4096, 4096),      # Second linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Dropout(0.5),            # Dropout with p=0.5\n",
    "    nn.Linear(4096, 2)          # Output layer with output size of 2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "Classifier3 = Classifier(model, criterion, optimizer)\n",
    "Classifier3.train()\n",
    "Classifier3.validate()\n",
    "Classifier3.predict('VGG19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - State of the art : DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/quoniam/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Epoch 1: 100%|██████████| 201/201 [01:05<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.45611428554674877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 201/201 [01:09<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.3879722756755293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 201/201 [01:11<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.37896293735326225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 201/201 [01:05<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.35757238729231394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 201/201 [01:06<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.34731337984106436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 201/201 [01:05<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.344371156312933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 201/201 [01:05<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.32975243639886676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 201/201 [01:06<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.3149603682741597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 201/201 [01:06<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.3091469077180274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 201/201 [01:05<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.30815220894801676\n",
      "Training finished - Elapsed time: 667.7542994022369 s.\n",
      "Accuracy on validation set: 84.42492012779553%\n"
     ]
    }
   ],
   "source": [
    "dinov2 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "\n",
    "# freeze all parameters\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "# Define the sequential model with VGG and additional linear layers\n",
    "model = nn.Sequential(\n",
    "    dinov2,\n",
    "    nn.Linear(384, 512),   # First linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Linear(512, 256),        # Second linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Linear(256, 2)           # Output layer with output size of 2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "Classifier4 = Classifier(model, criterion, optimizer)\n",
    "Classifier4.train()\n",
    "Classifier4.validate()\n",
    "Classifier4.predict('VGG19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this takes time. Let's try to save vgg19 features and then loading them in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     38\u001b[0m Classifier2 \u001b[38;5;241m=\u001b[39m Classifier(model, criterion, optimizer,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.features/vgg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mClassifier2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m Classifier2\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m     41\u001b[0m Classifier2\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG19_saved_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 67\u001b[0m, in \u001b[0;36mClassifier.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 67\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining finished - Elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_features(loader,name,model,model_name):\n",
    "    if os.path.exists(f'.features/{model_name}/{name}.pt'):\n",
    "        return torch.load(f'.features/{model_name}/{name}.pt')\n",
    "    \n",
    "    features = []\n",
    "    for batch, _ in tqdm(loader, desc=f'Extracting features from {name}'):\n",
    "        batch = batch.to(device)\n",
    "        # Extract the features\n",
    "        feature_batch = model(batch)\n",
    "        features.append(feature_batch)\n",
    "    features = torch.cat(features)\n",
    "    if not os.path.exists('.features'):\n",
    "        os.makedirs('.features')\n",
    "    if not os.path.exists(f'.features/{model_name}'):\n",
    "        os.makedirs(f'.features/{model_name}')\n",
    "    torch.save(features,f'.features/{model_name}/{name}.pt')\n",
    "    return features\n",
    "\n",
    "train_features = get_features(train_loader,'train',vgg,'vgg')\n",
    "val_features = get_features(val_loader,'val',vgg,'vgg')\n",
    "test_features = get_features(test_loader,'test',vgg,'vgg')\n",
    "\n",
    "# Define the sequential model with VGG and additional linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),               # Flatten the output of VGG\n",
    "    nn.Linear(25088, 4096),     # First linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Dropout(0.5),            # Dropout with p=0.5\n",
    "    nn.Linear(4096, 4096),      # Second linear layer\n",
    "    nn.ReLU(),                  # ReLU activation\n",
    "    nn.Dropout(0.5),            # Dropout with p=0.5\n",
    "    nn.Linear(4096, 2)          # Output layer with output size of 2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "Classifier2 = Classifier(model, criterion, optimizer,'.features/vgg')\n",
    "Classifier2.train()\n",
    "Classifier2.validate()\n",
    "Classifier2.predict('VGG19_saved_features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
